---
title: "WindowsでOllamaとClineを使ったローカルLLM開発環境の構築"
slug: "windows-local-llm-ollama-cline"
publishedAt: "2025-08-08T00:00:00Z"
updatedAt: "2025-08-08T00:00:00Z"
tags: ["ollama", "cline", "llm", "vscode"]
description: "メモリ32GBのWindows PCでgpt-oss:20bモデルを使ったローカルLLM開発環境をOllamaとClineで構築する手順を詳しく解説します。"
emoji: "🤖"
---

最近、ローカルでLLMを動かす需要が高まっています。プライバシーを重視したい場面や、インターネット接続が不安定な環境での開発など、様々な理由でローカルLLMが注目されています。

今回は、Windows環境でOllamaとVSCodeのCline拡張機能を使って、gpt-oss:20bモデルを動かすローカルLLM開発環境を構築する方法をご紹介します。メモリ32GBのPCで試してみました。

## 前提条件

この記事で試した環境は以下の通りです：

- Windows 11
- メモリ32GB
- 十分なストレージ容量（モデルファイルで数GB必要）
- VSCode がインストール済み

## Ollamaのインストール

まずは、ローカルLLMの実行環境となるOllamaをインストールしましょう。

### 1. インストーラーのダウンロード

Ollamaの公式サイト（https://ollama.com/）にアクセスし、「Download for Windows」からインストーラーをダウンロードします。

### 2. インストール実行

ダウンロードしたインストーラーを実行し、画面の指示に従ってインストールを完了させます。特別な設定は不要で、デフォルトの設定で問題ありません。

## gpt-oss:20bモデルのダウンロード

次に、実際に使用するLLMモデルをダウンロードします。

### 1. Ollamaアプリの起動

インストールが完了したら、Ollamaアプリを起動します。

### 2. モデルの選択とダウンロード

アプリ内でモデル選択画面から「gpt-oss:20b」を選択します。適当なメッセージを入力すると、自動的にモデルのダウンロードが開始されます。

20Bパラメータのモデルなので、ダウンロードには時間がかかります。
![image-download-model](image-download-model.png)

### 3. コンテキスト長の設定

ダウンロード完了後、Settings > Context length で、Context length を 4k（デフォルト値）に設定します。メモリに余裕がある場合は、後で調整することも可能です。

### 4. システムの再起動

設定を確実に反映させるため、PCを一度再起動します。

## インストール後の動作確認

Ollamaが正しくインストールされているか確認しましょう。

PowerShellを開き（一度閉じて再度開く）、以下のコマンドを実行します：

```powershell
ollama --version
```

バージョン番号が表示されれば、インストールは成功です。

```
ollama version is 0.11.4
```

のような出力が得られるはずです。

## VSCodeにClineをインストール

続いて、VSCodeにCline拡張機能をインストールして、LLMとの連携環境を整えます。

### 1. Cline拡張機能のインストール

VSCodeを起動し、拡張機能タブから「Cline」を検索してインストールします。

### 2. Clineの設定

インストール完了後、Clineの設定を行います。以下の設定値を使用します：

```
API Provider: Ollama
Model: gpt-oss:20b
Model Context Window: 4096
Request Timeout: 300000
```

各設定項目の詳細：

- **API Provider**: Ollamaを選択
- **Model**: gpt-oss:20bを指定
- **Model Context Window**: Ollamaの設定に合わせて4096に設定
- **Request Timeout**: 300000ms（5分）に設定

## 設定のポイント

### コンテキストウィンドウサイズについて

Context Window のサイズは、OllamaとClineの両方で一致させる必要があります。今回は4096に設定していますが、PCのスペックが高い場合は大きいサイズに設定します。

### タイムアウト設定について

ローカルLLMは、クラウドベースのAPIと比較して応答が遅くなる傾向があります。そのため、Request Timeoutは長めに設定することを強く推奨します。


## 実際の使用感

### パフォーマンスについて

正直なところ、Clineでローカルgpt-oss:20bを使用すると、かなり動作が遅いです。クラウドベースのGPT-4やClaude 3.5 Sonnetと比較すると、以下のような違いがあります：

**応答速度の比較**
- クラウドAPI: 数秒〜十数秒
- ローカルgpt-oss:20b: 数十秒〜数分

### 適用場面

この環境が特に有効なのは以下のような場面です：

- **プライバシーが重要なプロジェクト**: コードや情報を外部に送信したくない場合
- **インターネット接続が不安定**: オフライン環境での開発
- **学習・実験目的**: LLMの動作を詳しく理解したい場合
- **コスト削減**: API使用料を抑えたい長期プロジェクト

## まとめ

WindowsでOllamaとClineを使ったローカルLLM環境の構築は、思っているより簡単に実現できます。ただし、パフォーマンス面では正直なところ、かなり遅いというのが実情です。

**メリット**
- プライバシーの完全な保護
- インターネット接続不要
- API使用料金なし
- 学習・実験に最適

**デメリット**
- 応答速度が非常に遅い
- 大量のメモリとストレージが必要
- 電力消費が大きい

クラウドAPIの代替として日常的に使うには厳しいものがありますが、特定の用途では非常に価値のある環境だと思います。特に、機密性の高いプロジェクトや、LLMの動作原理を深く理解したい場合には、この環境での経験が役立つかもしれません。

興味のある方は、ぜひ一度試してみてください。思ったより簡単にセットアップできるので、週末のプロジェクトとしても面白いかもしれません。

> **注意**: この記事の内容は2025年8月時点の情報に基づいています。OllamaやClineのバージョンアップにより、設定方法や動作が変更される可能性があります。